作者：腾讯云技术社区
链接：https://zhuanlan.zhihu.com/p/28534295
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

周志华教授是蜚声国内外的机器学习专家，也是本届中国人工智能大会的主席之一。他的《机器学习》2016年1月出版之后，迅速成为这个领域的一本权威教材，在一年半的时间里重印十几次，发行逾16万册，并被冠以“西瓜书”的昵称，成为这一轮 AI 热潮的一个重要注脚。周志华教授潜心学术，为人低调，极少接受采访。这次中国人工智能大会上，由会议安排，他破例接受了我们的专访，就很多重要问题坦率的谈了自己的看法。我们特将内容整理成文，以飨读者。大反转难免会有，盲目追捧深度学习有危险AI科技大本营：感谢周教授接受采访。先谈谈当下最火的深度学习。河南大学的张重生教授在他的《深度学习》一书里摘录了你的一段话，是这么说的：有点幽默，但很朴实，深度学习现在差不多就是民工活，调来调去，刷来刷去。文章发得飞快，貌似热闹，但有多少是能积淀下来的实质真进展，又有多少是换个数据就不靠谱了的蒙事撞大运？既缺乏清澈干净的内在美感，又不致力于去伪存真、正本清源，只图热闹好看，迟早把 arXiv 变成废纸堆。看来，对今天深度学习火爆的局面，您有您的不以为然。能否详细的解释一下您的观点？周志华：不要误会，深度学习技术本身确实非常有用，能解决很多难题。有问题的是我看到国内的一种态势，什么东西一热起来，大家一拥而上，把所有其他东西全部都忽略了，好像机器学习乃至人工智能里面只有深度学习，这是很大的问题。深度学习中间还有很多困难而又重要的问题值得深入研究，但这些真正值得研究的问题，就我看到的情况而言，好像做的人非常少。大多数人在干什么呢？拿它做做应用，调调参数，性能刷几个点，然后发几篇文章。这样虽然容易发表文章，但恐怕很难产生有影响的成果。工业界倒是没什么，不管用什么技术，产品性能提升了就好，但是学术界这样就不太正常了，都去做深度学习的应用，不去研究深度学习机器学习里面更本质的问题，我担心很多年轻人的聪明才智被耽误了。另一方面，如果所有的人都只看到深度学习，忽略了其他研究内容，这是相当危险的。例如我们回顾一下，2010年左右的时候，有多少人意识到神经网络技术很有用了？很少。但其实这方面早在2005年左右的时候就已经突破了。那么当时为什么意识不到呢？因为大家都在追热门，而神经网络当时是冷门。这样的事情会不会重演？也许10年之后很重要的技术，今天已经有苗头了，如果我们全都扑向深度学习，根本不去关心其他的东西，那会不会把未来丢掉了？AI科技大本营：现在很多人都想学习深度学习，沾沾仙气。一般来说学习深度学习，总还是要具备一个比较完整的机器学习基础。但现在很多人不学机器学习基础课，一上来就搞深度学习，好像也能学得会，您赞成这种速成法吗？周志华：这要看学习的目的是什么。如果仅仅是为了使用现有的深度学习工具去做应用、解决已经清楚定义好的任务，那么可以去学速成法。只不过，这样的工作很容易被其他人替代。如果学习的目的是为了深入理解，为了在深度学习的研究里面有自己的创新，或者为了自如地解决那些没人给你清楚定义好的任务，那么恐怕还是要从打好基础开始。这就像武侠小说里的“正宗功夫”要慢慢打基础，“邪门功夫”上手快、短期内更“厉害”，但是到了后来，邪门功夫到了一定程度就会上不去了。AI科技大本营：也就是说您认为深度学习是对传统方法的一次反转，而未来还会有别的方法来反转深度学习。您觉得这个可能性有多大？周志华：“反转”这个词未必合适，但是必然会有更强的技术出现。AI科技大本营：会是什么？概率图模型还有戏吗？周志华：我们没法准确的“预知未来”。我想我们应该去关注一些重要的问题，这些问题在什么时候能得到解决，就可能带来新的突破。这涉及到很多其他方面的因素甚至机遇，没法能够事先规划出来下一个东西爆发点在哪。所以我觉得我们千万不要把眼光局限在一个地方。至于概率图模型，它有它的短板，也有它擅长的问题。AI科技大本营：我知道您偏爱集成学习方法。现在的随机森林算法、XGBoost 算法，效果很好，非常流行。不过我们都知道机器学习有所谓的“No free lunch theorem”，也就是说对于一般的问题，任意两种机器学习算法的期望性能都相同，没有哪一种方法是有绝对优势的。可是现实情况是，在 Kaggle 和其他的机器学习大赛当中，深度学习和 XGBoost 基本上一统天下。这种情况为什么会出现？周志华：机器学习技术解决现实问题的时候，通常要“度身定做”，根据问题的特性去对方法做改造、甚至设计出专门的新方法，才能有更好的效果。但是能这样做的人不多，需要很资深的专家。Kaggle之类的比赛里面，普通级别的玩家比较多，这时候比较热门流行的技术就比较容易被关注、被尝试，并且深度学习和XGBoost都有现成的工具，很容易就被拿来用。不过我们应该意识到，在某个比赛上获得优胜的方法，未必是在这个任务上最优秀的技术。因为比赛的名次和很多东西有关，例如两种不同的方法，使用的人经验不同、花的功夫不同、使用的资源不同，最后的名次是因为方法本身的差别导致的，还是其他因素导致的，很难说。AI科技大本营：您前一段时间发表了一篇文章，提出了 Deep Forest 算法，引起广泛的关注。它是怎么出来的？现在进行到什么阶段了？周志华：这个工作是希望能打开一个新的方向。今天在很多涉及到图像、声音的任务中，深度学习表现很出色。我们可以从两方面来看。一方面，现在当大家说到深度学习的时候，基本上就是在谈深度神经网络，甚至很多人认为深度学习只能通过神经网络来做。那么，深度学习是不是只能通过神经网络做？用别的结构行不行？这在理论上是很重要的一个问题。因为神经网络基本部件是可微分的，这直接导致了后续误差逆传播等技术选择，而现实世界当中并不是所有的规律都是平滑可微分的，非要用可微分的部件来建模，一定是最佳路径吗？而且机器学习中有很多类型的部件，其中相当一部分是不可微分的，基于这些部件能不能进行深度学习？这些问题我们希望探索一下，可能会产生新的启发。另一方面，从应用的角度看，虽然今天深度神经网络在很多任务上性能很好，但仍然有很多任务，像随机森林这样的技术表现出色，甚至比深度学习效果还好。那么，如果能做出深度森林来，会不会在这些任务上有更好的结果呢？从某种意义上说，相当于是把深度学习的适用范围推广到更多的任务上去，这是它在应用上的价值。就这个工作来说，我们更关心的是这条路能不能走，毕竟这是一条新的道路。如果能走，性能改进和效率提升是后面的事，有很大的空间。至于能走多远，现在我们也不清楚。就像20来年前，卷积神经网络刚出来的时候，没人预料到它后来掀起了深度学习的热潮。AI科技大本营：所以 Deep Forest 不是小打小闹，是您战略性的研究方向，是不是这样？周志华：这个工作确实考虑了很长时间。它主要是关于我关心的两个问题，第一个是基于不可微分的部件怎么做成深度模型，第二是对于集成方法和基于树的结构，我们有没有办法通过引入深度学习的思想把它的性能做的更好。这两个在机器学习中应该算是比较重要和基本的问题。西瓜书是希望帮助读者了解机器学习的全貌AI科技大本营：我们谈您的书，现在大家昵称为西瓜书，我买的时候发行了7万册，现在有10万册了吧。